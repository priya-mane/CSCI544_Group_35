{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rake-nltk\n!pip install gensim==3.6.0\n!pip install yake","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-02T03:38:59.504501Z","iopub.execute_input":"2022-12-02T03:38:59.505588Z","iopub.status.idle":"2022-12-02T03:40:18.243987Z","shell.execute_reply.started":"2022-12-02T03:38:59.505360Z","shell.execute_reply":"2022-12-02T03:40:18.242391Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rake-nltk\n  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\nRequirement already satisfied: nltk<4.0.0,>=3.6.2 in /opt/conda/lib/python3.7/site-packages (from rake-nltk) (3.7)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.64.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2021.11.10)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.0.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk<4.0.0,>=3.6.2->rake-nltk) (4.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk<4.0.0,>=3.6.2->rake-nltk) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk<4.0.0,>=3.6.2->rake-nltk) (4.4.0)\nInstalling collected packages: rake-nltk\nSuccessfully installed rake-nltk-1.0.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting gensim==3.6.0\n  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim==3.6.0) (1.21.6)\nRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim==3.6.0) (1.7.3)\nRequirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim==3.6.0) (1.15.0)\nRequirement already satisfied: smart_open>=1.2.1 in /opt/conda/lib/python3.7/site-packages (from gensim==3.6.0) (5.2.1)\nBuilding wheels for collected packages: gensim\n  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-linux_x86_64.whl size=24613059 sha256=6cfe1f838b3392941cf780f9ecd4b0a6876a221df710139e3359862e4acd7aac\n  Stored in directory: /root/.cache/pip/wheels/53/c8/f9/afb722099bdb5d73e5807019ce1512fd065502ccc15ea2b5bd\nSuccessfully built gensim\nInstalling collected packages: gensim\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.0.1\n    Uninstalling gensim-4.0.1:\n      Successfully uninstalled gensim-4.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscattertext 0.1.7 requires gensim>=4.0.0, but you have gensim 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gensim-3.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting yake\n  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m732.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from yake) (0.9.0)\nRequirement already satisfied: click>=6.0 in /opt/conda/lib/python3.7/site-packages (from yake) (8.0.4)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from yake) (2.5)\nCollecting jellyfish\n  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from yake) (1.21.6)\nCollecting segtok\n  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click>=6.0->yake) (4.13.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->yake) (5.1.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from segtok->yake) (2021.11.10)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click>=6.0->yake) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click>=6.0->yake) (3.8.0)\nBuilding wheels for collected packages: jellyfish\n  Building wheel for jellyfish (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=102035 sha256=d5dedb1622257ba0093bc3ff01bcc2b954e2c01ad100941598ba2bcea8b04e35\n  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\nSuccessfully built jellyfish\nInstalling collected packages: segtok, jellyfish, yake\nSuccessfully installed jellyfish-0.9.0 segtok-1.5.11 yake-0.4.8\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom rake_nltk import Rake\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom operator import itemgetter\nimport networkx as nx\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing\n\nnltk_stop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:40:18.246277Z","iopub.execute_input":"2022-12-02T03:40:18.247003Z","iopub.status.idle":"2022-12-02T03:40:20.026423Z","shell.execute_reply.started":"2022-12-02T03:40:18.246963Z","shell.execute_reply":"2022-12-02T03:40:20.025437Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_text(txt):\n    txt = txt.strip('][').split(', ')\n    clean_txt = []\n    for sent in txt:\n        s = re.sub(r'[^\\w\\s]', '', sent)\n        if len(s)>0:\n            s += \".\"\n            clean_txt.append(s)\n    \n    final_txt = []\n    for c in clean_txt:\n        words = word_tokenize(c.lower())\n        cleaned_words = []\n        for w in words:\n            if w not in nltk_stop_words:\n                cleaned_words.append(w)\n        final_txt.append(\" \".join(cleaned_words))\n    return \". \".join(final_txt)\n\n\ndef clean_transcription_text(txt):\n    s = re.sub(r'[^\\w\\s]', '', txt)\n    words = word_tokenize(s.lower())\n    cleaned_words = []\n    for w in words:\n        if w not in nltk_stop_words:\n            cleaned_words.append(w)\n    final_txt = []   \n    final_txt.append(\" \".join(cleaned_words))\n    return \". \".join(final_txt)\n\n\ndef get_stop_words(cleaned_symptoms, min_count=5, max_count=90):\n    cleaned_sym_list = [word_tokenize(sent.lower()) for sent in cleaned_symptoms.tolist()]\n    model = Word2Vec(sentences=cleaned_sym_list, size=1000, window=5, min_count=1, workers=4)\n    model.save(\"word2vec.model\")\n    stop_words = set()\n    word_list = []\n    for w in model.wv.vocab:\n        word_list.append( (w, model.wv.vocab[w].count) )\n    word_list.sort(key=lambda x:x[1], reverse=True)\n    \n    print(\"Total vocabulary = \", len(word_list))\n    for w,c in word_list:\n        if c<=min_count or c>=max_count:\n            stop_words.add(w)\n            \n    print(\"identified \", len(stop_words),  \" stopwords.\")\n    \n    return stop_words, model\n\ndef remove_stop_words(x, stop_words):\n    words = word_tokenize(x)\n    clean_words = []\n    for w in words:\n        if (w not in stop_words) :\n            clean_words.append(w)\n    return \" \".join(clean_words)\n\n\ndef get_keywords(text):\n    r = Rake()\n    r.extract_keywords_from_text(text)\n    list_terms = list(set(r.get_ranked_phrases()))\n    keywords = set()\n    for sent in list_terms:\n        words = word_tokenize(sent)\n        keywords.update(words)\n    return list(keywords)\n\ndef get_vector(model, keywords):\n    word_vectors = []\n    for w in keywords:\n        v = model.wv[w].reshape(1,-1)\n        word_vectors.append(v)\n        \n    word_vectors = np.array(word_vectors)\n    mean_vec = np.mean(word_vectors, axis=0)\n    return mean_vec","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:40:20.028394Z","iopub.execute_input":"2022-12-02T03:40:20.028874Z","iopub.status.idle":"2022-12-02T03:40:20.047989Z","shell.execute_reply.started":"2022-12-02T03:40:20.028838Z","shell.execute_reply":"2022-12-02T03:40:20.046726Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# disease_components = pd.read_csv(\"/kaggle/input/diseases-dataset/disease_components.csv\", encoding='latin1')\n# disease_components.dropna(inplace=True)\n# disease_components['cleaned_symptoms'] = disease_components.apply(lambda x: clean_text(x['Symptoms']), axis=1)\n# stop_words, word2ec_model = get_stop_words(disease_components['cleaned_symptoms'], min_count=5, max_count=100)\n# disease_components['cleaned_symptoms'] = disease_components.apply(lambda x: remove_stop_words(x['cleaned_symptoms'], stop_words), axis=1)\n# disease_components['keywords'] = disease_components.apply(lambda x: get_keywords(x['cleaned_symptoms']), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:40:20.052779Z","iopub.execute_input":"2022-12-02T03:40:20.053199Z","iopub.status.idle":"2022-12-02T03:40:20.066627Z","shell.execute_reply.started":"2022-12-02T03:40:20.053142Z","shell.execute_reply":"2022-12-02T03:40:20.065213Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# disease_components['symptom_vector'] = disease_components.apply(lambda x: get_vector(word2ec_model, x['keywords']), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:40:20.068068Z","iopub.execute_input":"2022-12-02T03:40:20.068400Z","iopub.status.idle":"2022-12-02T03:40:20.081750Z","shell.execute_reply.started":"2022-12-02T03:40:20.068372Z","shell.execute_reply":"2022-12-02T03:40:20.080424Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MT Samples dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/medicaltranscriptions/mtsamples.csv\")\ndf = df[['medical_specialty', 'transcription']]","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:40:20.083760Z","iopub.execute_input":"2022-12-02T03:40:20.084449Z","iopub.status.idle":"2022-12-02T03:40:20.510582Z","shell.execute_reply.started":"2022-12-02T03:40:20.084413Z","shell.execute_reply":"2022-12-02T03:40:20.509074Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ndf['medical_specialty_encoded'] = le.fit_transform(df['medical_specialty'])\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:40:20.512127Z","iopub.execute_input":"2022-12-02T03:40:20.512802Z","iopub.status.idle":"2022-12-02T03:40:20.529392Z","shell.execute_reply.started":"2022-12-02T03:40:20.512767Z","shell.execute_reply":"2022-12-02T03:40:20.528025Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df['transcription_cleaned'] = df.apply(lambda x: clean_transcription_text(x['transcription']), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:40:20.530756Z","iopub.execute_input":"2022-12-02T03:40:20.531098Z","iopub.status.idle":"2022-12-02T03:40:30.217784Z","shell.execute_reply.started":"2022-12-02T03:40:20.531068Z","shell.execute_reply":"2022-12-02T03:40:30.216223Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"stop_words, word2ec_model = get_stop_words(df['transcription_cleaned'], min_count=5, max_count=100)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:40:30.220002Z","iopub.execute_input":"2022-12-02T03:40:30.220572Z","iopub.status.idle":"2022-12-02T03:41:08.314476Z","shell.execute_reply.started":"2022-12-02T03:40:30.220504Z","shell.execute_reply":"2022-12-02T03:41:08.313229Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Total vocabulary =  44687\nidentified  32340  stopwords.\n","output_type":"stream"}]},{"cell_type":"code","source":"df['transcription_cleaned'] = df.apply(lambda x: remove_stop_words(x['transcription_cleaned'], stop_words), axis=1)\ndf['transcription_keywords'] = df.apply(lambda x: get_keywords(x['transcription_cleaned']), axis=1)\ndf['symptom_vector'] = df.apply(lambda x: get_vector(word2ec_model, x['transcription_keywords']), axis=1)\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:08.319124Z","iopub.execute_input":"2022-12-02T03:41:08.320091Z","iopub.status.idle":"2022-12-02T03:41:28.146403Z","shell.execute_reply.started":"2022-12-02T03:41:08.320050Z","shell.execute_reply":"2022-12-02T03:41:28.145180Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n  out=out, **kwargs)\n/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n","output_type":"stream"}]},{"cell_type":"code","source":"vectors = [k.tolist()[0] for k in df['symptom_vector'].tolist()]\ndf_final = pd.DataFrame(vectors, columns=[\"f_\"+str(i) for i in range(1000)])\ndf_final['target'] = df['medical_specialty_encoded'].tolist()\nX = df_final.drop(columns=['target'])\ny = df_final['target']","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:28.147940Z","iopub.execute_input":"2022-12-02T03:41:28.148388Z","iopub.status.idle":"2022-12-02T03:41:29.734526Z","shell.execute_reply.started":"2022-12-02T03:41:28.148332Z","shell.execute_reply":"2022-12-02T03:41:29.733447Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:29.736083Z","iopub.execute_input":"2022-12-02T03:41:29.736535Z","iopub.status.idle":"2022-12-02T03:41:29.778779Z","shell.execute_reply.started":"2022-12-02T03:41:29.736485Z","shell.execute_reply":"2022-12-02T03:41:29.777467Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', max_iter=10000).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nf1_ = f1_score(y_test, y_pred, average='weighted')\nprint(\"accuracy = \", acc)\nprint(\"F1 score = \", f1_)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:29.780260Z","iopub.execute_input":"2022-12-02T03:41:29.780898Z","iopub.status.idle":"2022-12-02T03:41:44.866759Z","shell.execute_reply.started":"2022-12-02T03:41:29.780862Z","shell.execute_reply":"2022-12-02T03:41:44.864702Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"accuracy =  0.35683629675045986\nF1 score =  0.26818154736675454\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nclf = GaussianNB().fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nf1_ = f1_score(y_test, y_pred, average='weighted')\nprint(\"accuracy = \", acc)\nprint(\"F1 score = \", f1_)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:44.869878Z","iopub.execute_input":"2022-12-02T03:41:44.872719Z","iopub.status.idle":"2022-12-02T03:41:45.423180Z","shell.execute_reply.started":"2022-12-02T03:41:44.872645Z","shell.execute_reply":"2022-12-02T03:41:45.422145Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"accuracy =  0.1894543225015328\nF1 score =  0.1829017368425353\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom sklearn.linear_model import SGDClassifier\n\nclf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nf1_ = f1_score(y_test, y_pred, average='weighted')\nprint(\"accuracy = \", acc)\nprint(\"F1 score = \", f1_)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:45.424967Z","iopub.execute_input":"2022-12-02T03:41:45.425370Z","iopub.status.idle":"2022-12-02T03:41:46.871064Z","shell.execute_reply.started":"2022-12-02T03:41:45.425336Z","shell.execute_reply":"2022-12-02T03:41:46.865621Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"accuracy =  0.21949724095646841\nF1 score =  0.20826109936886789\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nclf = LinearSVC(random_state=0, tol=1e-5).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nf1_ = f1_score(y_test, y_pred, average='weighted')\nprint(\"accuracy = \", acc)\nprint(\"F1 score = \", f1_)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:42:34.063450Z","iopub.execute_input":"2022-12-02T03:42:34.063898Z","iopub.status.idle":"2022-12-02T03:43:29.398616Z","shell.execute_reply.started":"2022-12-02T03:42:34.063863Z","shell.execute_reply":"2022-12-02T03:43:29.396985Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"accuracy =  0.3703249540159411\nF1 score =  0.2873525152146599\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main_df = pd.read_csv(\"/kaggle/input/diseases-and-symptoms/main.csv\")\n\n# disease_symp = defaultdict(list)\n\n# def clean_data(x):\n#     conditions = x['label'].split('^')\n#     conditions = [ k.split('_')[1] for k in conditions]\n    \n#     symptoms = []\n#     for symp in x.keys():\n#         if symp!='label' and symp!='frequency' and x[symp]==1:\n#             symptoms.append( symp.split('_')[1] )\n            \n#     for c in conditions:\n#         disease_symp[c.lower()] = symptoms\n        \n    \n# _ = main_df.apply(lambda x: clean_data(x), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:46.874802Z","iopub.execute_input":"2022-12-02T03:41:46.876047Z","iopub.status.idle":"2022-12-02T03:41:46.888818Z","shell.execute_reply.started":"2022-12-02T03:41:46.875975Z","shell.execute_reply":"2022-12-02T03:41:46.886510Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# diseases = set(disease_symp.keys())\n\n# big_dataset = set(disease_components['*'].tolist())\n# big_dataset = [k.lower() for k in big_dataset]\n\n# diseases.intersection(big_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:46.897861Z","iopub.execute_input":"2022-12-02T03:41:46.898610Z","iopub.status.idle":"2022-12-02T03:41:46.907325Z","shell.execute_reply.started":"2022-12-02T03:41:46.898533Z","shell.execute_reply":"2022-12-02T03:41:46.905555Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# def get_bigram_network(textlist):\n#     G = nx.Graph()\n#     for text in textlist:\n#         words = word_tokenize(text.lower())\n#         for i in range(len(words)-1):\n#             a = words[i]\n#             b = words[i+1]\n#             G.add_edge(a, b)\n        \n#     return G\n\n# G = get_bigram_network(disease_components['cleaned_symptoms'].tolist())\n# G.number_of_nodes()","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:46.910798Z","iopub.execute_input":"2022-12-02T03:41:46.912128Z","iopub.status.idle":"2022-12-02T03:41:46.920148Z","shell.execute_reply.started":"2022-12-02T03:41:46.912052Z","shell.execute_reply":"2022-12-02T03:41:46.918409Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# def knn(graph, node, n):\n#     return list(map(itemgetter(1),\n#                     sorted([(e)\n#                             for e in graph.edges(node, data=True)])[:n]))\n\n# node_degree = []\n# for n in G.nodes:\n#     if G.degree[n]!=0:\n#         node_degree.append((n, G.degree[n]))\n        \n# node_degree.sort(key = lambda x : x[1], reverse=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:46.923158Z","iopub.execute_input":"2022-12-02T03:41:46.925434Z","iopub.status.idle":"2022-12-02T03:41:46.937869Z","shell.execute_reply.started":"2022-12-02T03:41:46.925358Z","shell.execute_reply":"2022-12-02T03:41:46.936056Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# from gensim.summarization import keywords\n# print(keywords(cleaned_text))\n\n# import yake\n# kw_extractor = yake.KeywordExtractor()\n# language = \"en\"\n# max_ngram_size = 3\n# deduplication_threshold = 0.9\n# numOfKeywords = 20\n# custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n# keywords = custom_kw_extractor.extract_keywords(text)\n# for kw in keywords:\n#     print(kw)","metadata":{"execution":{"iopub.status.busy":"2022-12-02T03:41:46.941311Z","iopub.execute_input":"2022-12-02T03:41:46.943074Z","iopub.status.idle":"2022-12-02T03:41:46.953076Z","shell.execute_reply.started":"2022-12-02T03:41:46.942994Z","shell.execute_reply":"2022-12-02T03:41:46.950971Z"},"trusted":true},"execution_count":20,"outputs":[]}]}